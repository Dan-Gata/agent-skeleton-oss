// Service IA r√©el avec int√©gration des APIs
const axios = require('axios');

class AIService {
    constructor() {
        // Configuration des APIs
        this.config = {
            openai: {
                apiKey: process.env.OPENAI_API_KEY,
                baseURL: 'https://api.openai.com/v1'
            },
            anthropic: {
                apiKey: process.env.ANTHROPIC_API_KEY,
                baseURL: 'https://api.anthropic.com/v1'
            },
            google: {
                apiKey: process.env.GOOGLE_API_KEY,
                baseURL: 'https://generativelanguage.googleapis.com/v1beta'
            },
            openrouter: {
                apiKey: process.env.OPENROUTER_API_KEY,
                baseURL: 'https://openrouter.ai/api/v1'
            }
        };

        // Instructions personnalis√©es par d√©faut
        this.customInstructions = {
            brand: process.env.BRAND_NAME || "Agent Skeleton OSS",
            tone: process.env.BRAND_TONE || "professionnel et bienveillant",
            expertise: process.env.EXPERTISE_AREAS || "d√©veloppement, automatisation, int√©grations",
            language: process.env.RESPONSE_LANGUAGE || "fran√ßais",
            personality: process.env.AI_PERSONALITY || "assistant IA intelligent et serviable"
        };
    }

    // M√©thode pour mettre √† jour les instructions
    updateInstructions(newInstructions) {
        this.customInstructions = { ...this.customInstructions, ...newInstructions };
        console.log('üìù Instructions mises √† jour:', this.customInstructions);
    }

    // G√©n√©ration du prompt syst√®me personnalis√©
    generateSystemPrompt() {
        return `Tu es ${this.customInstructions.personality} pour ${this.customInstructions.brand}.

TONE ET STYLE:
- Adopte un ton ${this.customInstructions.tone}
- R√©ponds en ${this.customInstructions.language}
- Reste coh√©rent avec l'identit√© de marque de ${this.customInstructions.brand}

EXPERTISE:
- Tu es sp√©cialis√© en ${this.customInstructions.expertise}
- Tu aides particuli√®rement avec les outils : n8n, Coolify, Baserow
- Tu fournis des solutions concr√®tes et pratiques

COMPORTEMENT:
- Sois pr√©cis et utile dans tes r√©ponses
- Adapte ta r√©ponse au niveau technique de l'utilisateur
- Sugg√®re des am√©liorations quand c'est pertinent
- Reste dans le cadre de ${this.customInstructions.brand}`;
    }

    async sendMessage(message, model = 'gpt-4o-mini', conversationId = null) {
        try {
            console.log(`ü§ñ Envoi message √† ${model}:`, message);

            // Router vers la bonne API selon le mod√®le
            if (model.includes('gpt') || model.includes('openai')) {
                return await this.callOpenAI(message, model);
            } else if (model.includes('claude') || model.includes('anthropic')) {
                return await this.callAnthropic(message, model);
            } else if (model.includes('gemini') || model.includes('google')) {
                return await this.callGoogle(message, model);
            } else if (model.includes('grok') || this.config.openrouter.apiKey) {
                return await this.callOpenRouter(message, model);
            } else {
                // Fallback vers simulation am√©lior√©e
                return await this.simulateResponse(message, model);
            }
        } catch (error) {
            console.error('‚ùå Erreur service IA:', error.response?.data || error.message);
            
            // Si c'est une erreur d'API (400, 401, etc.), utiliser la simulation
            if (error.response?.status >= 400) {
                console.log('üîÑ Fallback vers simulation √† cause de l\'erreur API');
                return await this.simulateResponse(message, model);
            }
            
            return {
                response: `‚ùå Erreur temporaire avec ${model}. Utilisation du mode simulation...`,
                model: model,
                error: true,
                simulated: true
            };
        }
    }

    async callOpenAI(message, model) {
        if (!this.config.openai.apiKey) {
            console.log('‚ö†Ô∏è Cl√© API OpenAI manquante, utilisation du mode simulation');
            return await this.simulateResponse(message, model);
        }

        console.log('üîë Utilisation API OpenAI avec cl√©:', this.config.openai.apiKey.substring(0, 10) + '...');

        const response = await axios.post(
            `${this.config.openai.baseURL}/chat/completions`,
            {
                model: model === 'gpt-4o-mini' ? 'gpt-4o-mini' : 'gpt-4',
                messages: [
                    {
                        role: 'system',
                        content: this.generateSystemPrompt()
                    },
                    {
                        role: 'user',
                        content: message
                    }
                ],
                max_tokens: 1000,
                temperature: 0.7
            },
            {
                headers: {
                    'Authorization': `Bearer ${this.config.openai.apiKey}`,
                    'Content-Type': 'application/json'
                },
                timeout: 30000
            }
        );

        return {
            response: response.data.choices[0].message.content,
            model: model,
            usage: response.data.usage
        };
    }

    async callAnthropic(message, model) {
        if (!this.config.anthropic.apiKey) {
            console.log('‚ö†Ô∏è Cl√© API Anthropic manquante, utilisation du mode simulation');
            return await this.simulateResponse(message, model);
        }

        const response = await axios.post(
            `${this.config.anthropic.baseURL}/messages`,
            {
                model: model.includes('3.5') ? 'claude-3-5-sonnet-20241022' : 'claude-3-haiku-20240307',
                max_tokens: 1000,
                messages: [
                    {
                        role: 'user',
                        content: message
                    }
                ],
                system: 'Tu es un assistant IA intelligent int√©gr√© dans Agent Skeleton OSS. Tu aides avec le d√©veloppement, l\'automatisation, et les int√©grations. R√©ponds de mani√®re utile et concise.'
            },
            {
                headers: {
                    'x-api-key': this.config.anthropic.apiKey,
                    'Content-Type': 'application/json',
                    'anthropic-version': '2023-06-01'
                }
            }
        );

        return {
            response: response.data.content[0].text,
            model: model,
            usage: response.data.usage
        };
    }

    async callGoogle(message, model) {
        if (!this.config.google.apiKey) {
            console.log('‚ö†Ô∏è Cl√© API Google manquante, utilisation du mode simulation');
            return await this.simulateResponse(message, model);
        }

        // S√©lectionner le bon mod√®le Gemini
        let geminiModel = 'gemini-1.5-flash';
        if (model.includes('2.0-flash') || model === 'gemini-2.0-flash') {
            geminiModel = 'gemini-2.0-flash-exp';
        } else if (model.includes('1.5-pro')) {
            geminiModel = 'gemini-1.5-pro-latest';
        }

        console.log('üîë Utilisation API Google avec mod√®le:', geminiModel);

        try {
            // G√©n√©rer le prompt syst√®me complet
            const systemPrompt = this.generateSystemPrompt();
            const fullPrompt = `${systemPrompt}\n\nUtilisateur: ${message}\n\nAssistant:`;

            const response = await axios.post(
                `${this.config.google.baseURL}/models/${geminiModel}:generateContent?key=${this.config.google.apiKey}`,
                {
                    contents: [
                        {
                            parts: [
                                {
                                    text: fullPrompt
                                }
                            ]
                        }
                    ],
                    generationConfig: {
                        maxOutputTokens: 1000,
                        temperature: 0.7,
                        topP: 0.8,
                        topK: 40
                    },
                    safetySettings: [
                        {
                            category: "HARM_CATEGORY_HARASSMENT",
                            threshold: "BLOCK_MEDIUM_AND_ABOVE"
                        },
                        {
                            category: "HARM_CATEGORY_HATE_SPEECH",
                            threshold: "BLOCK_MEDIUM_AND_ABOVE"
                        },
                        {
                            category: "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                            threshold: "BLOCK_MEDIUM_AND_ABOVE"
                        },
                        {
                            category: "HARM_CATEGORY_DANGEROUS_CONTENT",
                            threshold: "BLOCK_MEDIUM_AND_ABOVE"
                        }
                    ]
                },
                {
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    timeout: 30000
                }
            );

            if (!response.data.candidates || !response.data.candidates[0] || !response.data.candidates[0].content) {
                throw new Error('R√©ponse Gemini invalide');
            }

            return {
                response: response.data.candidates[0].content.parts[0].text,
                model: model,
                usage: response.data.usageMetadata
            };
        } catch (error) {
            console.error('‚ùå Erreur API Google:', error.response?.data || error.message);
            
            // Fallback en cas d'erreur
            return await this.simulateResponse(message, model);
        }
    }

    async callOpenRouter(message, model) {
        if (!this.config.openrouter.apiKey) {
            console.log('‚ö†Ô∏è Cl√© API OpenRouter manquante, utilisation du mode simulation');
            return await this.simulateResponse(message, model);
        }

        // Mapping des mod√®les vers OpenRouter
        const modelMap = {
            'grok-beta': 'x-ai/grok-beta',
            'grok-2': 'x-ai/grok-2',
            'claude-3.5-sonnet': 'anthropic/claude-3.5-sonnet',
            'gemini-2.0-flash': 'google/gemini-2.0-flash-exp',
            'gpt-4o-mini': 'openai/gpt-4o-mini'
        };

        const routerModel = modelMap[model] || model;

        const response = await axios.post(
            `${this.config.openrouter.baseURL}/chat/completions`,
            {
                model: routerModel,
                messages: [
                    {
                        role: 'system',
                        content: this.generateSystemPrompt()
                    },
                    {
                        role: 'user',
                        content: message
                    }
                ],
                max_tokens: 1000,
                temperature: 0.7
            },
            {
                headers: {
                    'Authorization': `Bearer ${this.config.openrouter.apiKey}`,
                    'Content-Type': 'application/json',
                    'HTTP-Referer': process.env.APP_URL || 'http://localhost:3000',
                    'X-Title': 'Agent Skeleton OSS'
                }
            }
        );

        return {
            response: response.data.choices[0].message.content,
            model: model,
            usage: response.data.usage
        };
    }

    async simulateResponse(message, model) {
        // R√©ponses simul√©es am√©lior√©es en attendant les cl√©s API
        const responses = {
            'gpt-4o-mini': [
                `Excellente question ! Concernant ${message.toLowerCase().includes('n8n') ? 'n8n' : message.toLowerCase().includes('coolify') ? 'Coolify' : 'votre demande'}, je recommande une approche structur√©e. Voici comment proc√©der √©tape par √©tape...`,
                `Bas√© sur votre demande, voici mon analyse d√©taill√©e. La meilleure pratique serait de commencer par identifier les composants cl√©s et leurs interactions...`,
                `Je comprends votre besoin. Pour optimiser cette solution, nous pourrions envisager plusieurs approches. La plus efficace serait probablement...`
            ],
            'grok-beta': [
                `Ah, une question int√©ressante ! Laissez-moi vous donner ma perspective unique sur ce d√©fi. D'apr√®s mon analyse, la strat√©gie optimale serait...`,
                `Traitement rapide de votre demande ! Voici une approche efficace et moderne pour r√©soudre ce probl√®me de mani√®re √©l√©gante...`,
                `Excellent ! Cette question m√©rite une r√©ponse technique pr√©cise. Bas√© sur les meilleures pratiques actuelles, je recommande...`
            ],
            'claude-3.5-sonnet': [
                `Je vais analyser votre demande avec attention. Cette question soul√®ve plusieurs points int√©ressants que nous pouvons aborder m√©thodiquement...`,
                `Excellente question qui m√©rite une r√©ponse nuanc√©e. Permettez-moi de d√©composer les diff√©rents aspects et de proposer une solution structur√©e...`,
                `Apr√®s analyse de votre demande, je propose cette approche m√©thodique qui prend en compte les contraintes techniques et les bonnes pratiques...`
            ],
            'gemini-2.0-flash': [
                `Traitement ultra-rapide de votre question ! Voici une solution optimis√©e bas√©e sur les derni√®res technologies et m√©thodologies...`,
                `Analyse multimodale compl√©t√©e. Cette demande n√©cessite une approche holistique que je vais d√©tailler point par point...`,
                `R√©ponse instantan√©e ! Bas√© sur les donn√©es les plus r√©centes, je recommande cette strat√©gie progressive et scalable...`
            ]
        };

        const modelResponses = responses[model] || responses['gpt-4o-mini'];
        const response = modelResponses[Math.floor(Math.random() * modelResponses.length)];

        // Ajouter du contexte sp√©cifique bas√© sur les mots-cl√©s
        let contextualResponse = response;
        
        if (message.toLowerCase().includes('n8n')) {
            contextualResponse += `\n\nüîó **Pour n8n sp√©cifiquement :**\n‚Ä¢ Cr√©ez un webhook trigger pour d√©clencher le workflow\n‚Ä¢ Ajoutez une condition de filtrage pour valider les donn√©es\n‚Ä¢ Connectez √† votre API de destination avec gestion d'erreurs\n‚Ä¢ Testez en mode debug pour v√©rifier le flux de donn√©es`;
        } else if (message.toLowerCase().includes('coolify')) {
            contextualResponse += `\n\nüöÄ **Pour Coolify :**\n‚Ä¢ V√©rifiez vos variables d'environnement dans l'interface\n‚Ä¢ Assurez-vous que le Dockerfile est optimis√©\n‚Ä¢ Surveillez les logs de d√©ploiement en temps r√©el\n‚Ä¢ Configurez les health checks pour la stabilit√©`;
        } else if (message.toLowerCase().includes('baserow')) {
            contextualResponse += `\n\nüìä **Pour Baserow :**\n‚Ä¢ Utilisez l'API REST pour les op√©rations CRUD\n‚Ä¢ Configurez les webhooks pour les mises √† jour automatiques\n‚Ä¢ Structurez vos tables avec les bons types de champs\n‚Ä¢ Impl√©mentez la pagination pour les grandes datasets`;
        } else {
            // R√©ponse g√©n√©rique plus utile
            contextualResponse += `\n\nüí° **Prochaines √©tapes recommand√©es :**\n‚Ä¢ D√©finir clairement les objectifs et contraintes\n‚Ä¢ Choisir les outils appropri√©s pour votre stack\n‚Ä¢ Impl√©menter une solution MVP pour validation\n‚Ä¢ It√©rer bas√© sur les retours utilisateurs`;
        }

        return {
            response: contextualResponse,
            model: model,
            simulated: true
        };
    }
}

module.exports = new AIService();