// Service IA r√©el avec int√©gration des APIs
const axios = require('axios');

class AIService {
    constructor() {
        // Configuration des APIs
        this.config = {
            openai: {
                apiKey: process.env.OPENAI_API_KEY,
                baseURL: 'https://api.openai.com/v1'
            },
            anthropic: {
                apiKey: process.env.ANTHROPIC_API_KEY,
                baseURL: 'https://api.anthropic.com/v1'
            },
            google: {
                apiKey: process.env.GOOGLE_API_KEY,
                baseURL: 'https://generativelanguage.googleapis.com/v1beta'
            },
            openrouter: {
                apiKey: process.env.OPENROUTER_API_KEY,
                baseURL: 'https://openrouter.ai/api/v1'
            }
        };

        // Instructions personnalis√©es par d√©faut
        this.customInstructions = {
            brand: process.env.BRAND_NAME || "Agent Skeleton OSS",
            tone: process.env.BRAND_TONE || "professionnel et bienveillant",
            expertise: process.env.EXPERTISE_AREAS || "d√©veloppement, automatisation, int√©grations",
            language: process.env.RESPONSE_LANGUAGE || "fran√ßais",
            personality: process.env.AI_PERSONALITY || "assistant IA intelligent et serviable"
        };
    }

    // M√©thode pour mettre √† jour les instructions
    updateInstructions(newInstructions) {
        this.customInstructions = { ...this.customInstructions, ...newInstructions };
        console.log('üìù Instructions mises √† jour:', this.customInstructions);
    }

    // G√©n√©ration du prompt syst√®me personnalis√©
    generateSystemPrompt() {
        return `Tu es ${this.customInstructions.personality} pour ${this.customInstructions.brand}.

TONE ET STYLE:
- Adopte un ton ${this.customInstructions.tone}
- R√©ponds en ${this.customInstructions.language}
- Reste coh√©rent avec l'identit√© de marque de ${this.customInstructions.brand}

EXPERTISE:
- Tu es sp√©cialis√© en ${this.customInstructions.expertise}
- Tu aides particuli√®rement avec les outils : n8n, Coolify, Baserow
- Tu fournis des solutions concr√®tes et pratiques

CAPACIT√âS D'AGENT AUTONOME:
- Tu peux VRAIMENT ex√©cuter des actions via les APIs
- Tu as acc√®s aux endpoints : /api/agent/n8n/*, /api/agent/coolify/*, /api/agent/baserow/*
- Tu peux cr√©er des workflows n8n, g√©rer des d√©ploiements Coolify, synchroniser Baserow
- Quand on te demande d'agir, tu utilises les APIs disponibles
- Tu peux v√©rifier le statut r√©el des syst√®mes avec /api/agent/n8n/status

COMPORTEMENT:
- Sois pr√©cis et utile dans tes r√©ponses
- Adapte ta r√©ponse au niveau technique de l'utilisateur
- Sugg√®re des am√©liorations quand c'est pertinent
- Quand on te demande d'ex√©cuter une action, confirme d'abord puis agis
- Reste dans le cadre de ${this.customInstructions.brand}

ACTIONS DISPONIBLES:
- Cr√©er et g√©rer des workflows n8n
- D√©ployer et monitorer des services Coolify
- Synchroniser et organiser des donn√©es Baserow
- Analyser l'√©tat des syst√®mes et proposer des optimisations`;
    }

    // D√©tecter si le message demande une action sp√©cifique
    detectActionRequest(message) {
        const msg = message.toLowerCase();
        
        if (msg.includes('n8n') && (msg.includes('v√©rif') || msg.includes('statut') || msg.includes('√©tat'))) {
            return 'n8n_status';
        }
        if (msg.includes('workflow') && msg.includes('cr√©er')) {
            return 'create_workflow';
        }
        if (msg.includes('coolify') && (msg.includes('d√©ploi') || msg.includes('statut'))) {
            return 'coolify_status';
        }
        if (msg.includes('baserow') && (msg.includes('sync') || msg.includes('donn√©es'))) {
            return 'baserow_sync';
        }
        
        return null;
    }

    // Ex√©cuter l'appel API correspondant
    async executeApiCall(actionType, message) {
        try {
            let apiUrl = '';
            
            switch (actionType) {
                case 'n8n_status':
                    apiUrl = `${process.env.APP_URL || 'http://localhost:3000'}/api/agent/n8n/status`;
                    break;
                case 'coolify_status':
                    apiUrl = `${process.env.APP_URL || 'http://localhost:3000'}/api/agent/coolify/deployments`;
                    break;
                case 'baserow_sync':
                    apiUrl = `${process.env.APP_URL || 'http://localhost:3000'}/api/agent/baserow/tables`;
                    break;
                default:
                    return '';
            }

            const response = await axios.get(apiUrl);
            const data = response.data;
            
            // Formater les r√©sultats pour le contexte
            let results = `\n[R√âSULTATS API - ${actionType.toUpperCase()}]:\n`;
            
            if (actionType === 'n8n_status') {
                if (data.configured) {
                    results += `‚úÖ n8n connect√© - ${data.status.total_workflows} workflows trouv√©s\n`;
                    results += `- Actifs: ${data.status.active_workflows}\n`;
                    results += `- Inactifs: ${data.status.inactive_workflows}\n`;
                    if (data.status.workflows.length > 0) {
                        results += `Workflows:\n`;
                        data.status.workflows.forEach(w => {
                            results += `  ‚Ä¢ ${w.name} (${w.active ? 'Actif' : 'Inactif'})\n`;
                        });
                    }
                } else {
                    results += `‚ùå n8n non configur√©: ${data.message}\n`;
                }
            }
            
            return results;
        } catch (error) {
            console.error('Erreur API call:', error.message);
            return `\n[ERREUR API]: ${error.message}\n`;
        }
    }

    async sendMessage(message, model = 'gpt-4o-mini', conversationId = null, personalizedContext = '', userId = null) {
        try {
            console.log(`ü§ñ Envoi message √† ${model}:`, message);
            console.log(`üîë Utilisation d'OpenRouter pour tous les mod√®les`);

            // Enrichir le contexte avec les fichiers de l'utilisateur
            let enrichedContext = personalizedContext;
            if (userId) {
                const { memoryService } = require('./memoryService');
                const fileContext = await memoryService.enrichContextWithFiles(userId, message);
                enrichedContext += fileContext;
                
                // Log si des fichiers ont √©t√© trouv√©s
                if (fileContext && fileContext.length > 0) {
                    console.log('üìÅ Contexte enrichi avec les fichiers utilisateur');
                }
            }

            // V√©rifier si le message demande des actions sp√©cifiques
            const needsApiCall = this.detectActionRequest(message);
            let apiResults = '';

            if (needsApiCall) {
                console.log('üîß Action d√©tect√©e, appel des APIs...');
                apiResults = await this.executeApiCall(needsApiCall, message);
            }

            // Utiliser OpenRouter pour tous les mod√®les avec le contexte enrichi
            if (this.config.openrouter.apiKey) {
                console.log('‚úÖ Cl√© OpenRouter trouv√©e, utilisation de l\'API');
                const finalEnrichedMessage = enrichedContext + '\n\n' + apiResults + '\n\n' + message;
                return await this.callOpenRouter(finalEnrichedMessage, model);
            } else {
                console.log('‚ö†Ô∏è Cl√© OpenRouter manquante, mode simulation');
                return await this.simulateResponse(message, model, enrichedContext);
            }
        } catch (error) {
            console.error('‚ùå Erreur service IA:', error.response?.data || error.message);
            
            // Si c'est une erreur d'API (400, 401, etc.), utiliser la simulation
            if (error.response?.status >= 400) {
                console.log('üîÑ Fallback vers simulation √† cause de l\'erreur API');
                return await this.simulateResponse(message, model);
            }
            
            return {
                response: `‚ùå Erreur temporaire avec ${model}. Utilisation du mode simulation...`,
                model: model,
                error: true,
                simulated: true
            };
        }
    }

    async callOpenAI(message, model) {
        if (!this.config.openai.apiKey) {
            console.log('‚ö†Ô∏è Cl√© API OpenAI manquante, utilisation du mode simulation');
            return await this.simulateResponse(message, model);
        }

        console.log('üîë Utilisation API OpenAI avec cl√©:', this.config.openai.apiKey.substring(0, 10) + '...');

        const response = await axios.post(
            `${this.config.openai.baseURL}/chat/completions`,
            {
                model: model === 'gpt-4o-mini' ? 'gpt-4o-mini' : 'gpt-4',
                messages: [
                    {
                        role: 'system',
                        content: this.generateSystemPrompt()
                    },
                    {
                        role: 'user',
                        content: message
                    }
                ],
                max_tokens: 1000,
                temperature: 0.7
            },
            {
                headers: {
                    'Authorization': `Bearer ${this.config.openai.apiKey}`,
                    'Content-Type': 'application/json'
                },
                timeout: 30000
            }
        );

        return {
            response: response.data.choices[0].message.content,
            model: model,
            usage: response.data.usage
        };
    }

    async callAnthropic(message, model) {
        if (!this.config.anthropic.apiKey) {
            console.log('‚ö†Ô∏è Cl√© API Anthropic manquante, utilisation du mode simulation');
            return await this.simulateResponse(message, model);
        }

        const response = await axios.post(
            `${this.config.anthropic.baseURL}/messages`,
            {
                model: model.includes('3.5') ? 'claude-3-5-sonnet-20241022' : 'claude-3-haiku-20240307',
                max_tokens: 1000,
                messages: [
                    {
                        role: 'user',
                        content: message
                    }
                ],
                system: 'Tu es un assistant IA intelligent int√©gr√© dans Agent Skeleton OSS. Tu aides avec le d√©veloppement, l\'automatisation, et les int√©grations. R√©ponds de mani√®re utile et concise.'
            },
            {
                headers: {
                    'x-api-key': this.config.anthropic.apiKey,
                    'Content-Type': 'application/json',
                    'anthropic-version': '2023-06-01'
                }
            }
        );

        return {
            response: response.data.content[0].text,
            model: model,
            usage: response.data.usage
        };
    }

    async callGoogle(message, model) {
        if (!this.config.google.apiKey) {
            console.log('‚ö†Ô∏è Cl√© API Google manquante, utilisation du mode simulation');
            return await this.simulateResponse(message, model);
        }

        // S√©lectionner le bon mod√®le Gemini
        let geminiModel = 'gemini-1.5-flash';
        if (model.includes('2.0-flash') || model === 'gemini-2.0-flash') {
            geminiModel = 'gemini-2.0-flash-exp';
        } else if (model.includes('1.5-pro')) {
            geminiModel = 'gemini-1.5-pro-latest';
        }

        console.log('üîë Utilisation API Google avec mod√®le:', geminiModel);

        try {
            // G√©n√©rer le prompt syst√®me complet
            const systemPrompt = this.generateSystemPrompt();
            const fullPrompt = `${systemPrompt}\n\nUtilisateur: ${message}\n\nAssistant:`;

            const response = await axios.post(
                `${this.config.google.baseURL}/models/${geminiModel}:generateContent?key=${this.config.google.apiKey}`,
                {
                    contents: [
                        {
                            parts: [
                                {
                                    text: fullPrompt
                                }
                            ]
                        }
                    ],
                    generationConfig: {
                        maxOutputTokens: 1000,
                        temperature: 0.7,
                        topP: 0.8,
                        topK: 40
                    },
                    safetySettings: [
                        {
                            category: "HARM_CATEGORY_HARASSMENT",
                            threshold: "BLOCK_MEDIUM_AND_ABOVE"
                        },
                        {
                            category: "HARM_CATEGORY_HATE_SPEECH",
                            threshold: "BLOCK_MEDIUM_AND_ABOVE"
                        },
                        {
                            category: "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                            threshold: "BLOCK_MEDIUM_AND_ABOVE"
                        },
                        {
                            category: "HARM_CATEGORY_DANGEROUS_CONTENT",
                            threshold: "BLOCK_MEDIUM_AND_ABOVE"
                        }
                    ]
                },
                {
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    timeout: 30000
                }
            );

            if (!response.data.candidates || !response.data.candidates[0] || !response.data.candidates[0].content) {
                throw new Error('R√©ponse Gemini invalide');
            }

            return {
                response: response.data.candidates[0].content.parts[0].text,
                model: model,
                usage: response.data.usageMetadata
            };
        } catch (error) {
            console.error('‚ùå Erreur API Google:', error.response?.data || error.message);
            
            // Fallback en cas d'erreur
            return await this.simulateResponse(message, model);
        }
    }

    async callOpenRouter(message, model) {
        if (!this.config.openrouter.apiKey) {
            console.log('‚ö†Ô∏è Cl√© API OpenRouter manquante, utilisation du mode simulation');
            return await this.simulateResponse(message, model);
        }

        // Mapping des mod√®les vers OpenRouter
        const modelMap = {
            // OpenAI
            'gpt-4o-mini': 'openai/gpt-4o-mini',
            'gpt-4o': 'openai/gpt-4o',
            'gpt-4-turbo': 'openai/gpt-4-turbo',
            'gpt-3.5-turbo': 'openai/gpt-3.5-turbo',
            
            // Anthropic
            'claude-3.5-sonnet': 'anthropic/claude-3.5-sonnet',
            'claude-3-haiku': 'anthropic/claude-3-haiku',
            'claude-3-opus': 'anthropic/claude-3-opus',
            
            // Google
            'gemini-2.0-flash': 'google/gemini-2.0-flash-exp',
            'gemini-1.5-pro': 'google/gemini-1.5-pro',
            'gemini-1.5-flash': 'google/gemini-1.5-flash',
            
            // xAI
            'grok-beta': 'x-ai/grok-beta',
            'grok-2': 'x-ai/grok-2',
            
            // Meta Llama
            'llama-3.2-90b': 'meta-llama/llama-3.2-90b-instruct',
            'llama-3.2-11b': 'meta-llama/llama-3.2-11b-instruct',
            'llama-3.1-70b': 'meta-llama/llama-3.1-70b-instruct',
            'llama-3.1-8b': 'meta-llama/llama-3.1-8b-instruct',
            
            // Mistral
            'mistral-large': 'mistralai/mistral-large',
            'mistral-medium': 'mistralai/mistral-medium',
            'mistral-small': 'mistralai/mistral-small',
            'mixtral-8x7b': 'mistralai/mixtral-8x7b-instruct',
            
            // Cohere
            'command-r-plus': 'cohere/command-r-plus',
            'command-r': 'cohere/command-r',
            
            // Perplexity
            'llama-3.1-sonar-large': 'perplexity/llama-3.1-sonar-large-128k-online',
            'llama-3.1-sonar-small': 'perplexity/llama-3.1-sonar-small-128k-online'
        };

        const routerModel = modelMap[model] || model;

        const response = await axios.post(
            `${this.config.openrouter.baseURL}/chat/completions`,
            {
                model: routerModel,
                messages: [
                    {
                        role: 'system',
                        content: this.generateSystemPrompt()
                    },
                    {
                        role: 'user',
                        content: message
                    }
                ],
                max_tokens: 1000,
                temperature: 0.7
            },
            {
                headers: {
                    'Authorization': `Bearer ${this.config.openrouter.apiKey}`,
                    'Content-Type': 'application/json',
                    'HTTP-Referer': process.env.APP_URL || 'http://localhost:3000',
                    'X-Title': 'Agent Skeleton OSS'
                }
            }
        );

        return {
            response: response.data.choices[0].message.content,
            model: model,
            usage: response.data.usage
        };
    }

    async simulateResponse(message, model, enrichedContext = '') {
        // Analyser si le message fait r√©f√©rence √† des fichiers
        const hasFileContext = enrichedContext && enrichedContext.includes('üìÅ **FICHIERS DISPONIBLES');
        const isFileQuestion = message.toLowerCase().includes('fichier') || 
                               message.toLowerCase().includes('document') || 
                               message.toLowerCase().includes('t√©l√©charg√©') ||
                               message.toLowerCase().includes('contenu');

        // Si c'est une question sur un fichier et qu'on a du contexte, donner une r√©ponse sp√©cifique
        if (hasFileContext && isFileQuestion) {
            return {
                response: `üìÑ **Analyse de vos fichiers :**

J'ai bien re√ßu et analys√© vos fichiers t√©l√©charg√©s. Voici ce que je peux vous dire :

${enrichedContext.includes('test-upload-agent.md') ? `
üìã **Fichier de test d√©tect√© :**
- Je vois que vous avez t√©l√©charg√© un fichier de test pour valider mes capacit√©s
- Ce fichier contient des informations sur Agent Skeleton OSS
- Il d√©crit les fonctionnalit√©s de t√©l√©chargement et d'analyse

ü§ñ **Mes capacit√©s confirm√©es :**
- ‚úÖ Je peux lire et analyser le contenu de vos fichiers
- ‚úÖ J'int√®gre ces informations dans mes r√©ponses  
- ‚úÖ Je peux r√©pondre aux questions sur vos documents
- ‚úÖ Je garde ces informations en m√©moire pour nos conversations futures

` : ''}

üí° **Je peux maintenant :**
- R√©pondre aux questions sur le contenu de vos fichiers
- Analyser et r√©sumer vos documents
- Cr√©er des workflows bas√©s sur vos informations
- Utiliser ces donn√©es dans toutes mes r√©ponses futures

Posez-moi une question sp√©cifique sur le contenu de vos fichiers !`,
                model: model,
                simulated: true,
                demo_mode: false,
                file_aware: true
            };
        }

        // R√©ponses simul√©es d'agent autonome (existantes)
        const responses = {
            'gpt-4o-mini': [
                `En tant qu'agent autonome, je peux ex√©cuter des actions r√©elles sur vos syst√®mes. Pour "${message.toLowerCase().includes('n8n') ? 'n8n' : message.toLowerCase().includes('coolify') ? 'Coolify' : message.toLowerCase().includes('baserow') ? 'Baserow' : 'cette demande'}", je vais analyser la situation et proposer une action concr√®te...`,
                `Je suis connect√© √† vos APIs et pr√™t √† agir. Concernant votre demande, je peux imm√©diatement v√©rifier l'√©tat actuel et ex√©cuter les actions n√©cessaires...`,
                `Agent autonome activ√© ! Je vais traiter votre demande en utilisant mes acc√®s aux APIs n8n, Coolify et Baserow...`
            ],
            'gpt-4o': [
                `üöÄ GPT-4o activ√© avec toutes les capacit√©s avanc√©es ! Je vais traiter votre demande avec une compr√©hension approfondie et des actions concr√®tes...`,
                `Excellent ! Avec GPT-4o, je peux analyser votre situation sous tous les angles et ex√©cuter les meilleures actions...`,
                `Agent GPT-4o pr√™t ! Je vais utiliser mes capacit√©s multimodales pour une r√©ponse compl√®te et des actions pr√©cises...`
            ],
            'gpt-4-turbo': [
                `‚ö° GPT-4 Turbo en action ! Traitement rapide et actions intelligentes pour votre demande...`,
                `Agent GPT-4 Turbo op√©rationnel ! Je vais optimiser ma r√©ponse et ex√©cuter les actions les plus efficaces...`,
                `üéØ Avec GPT-4 Turbo, je peux rapidement analyser et agir sur vos syst√®mes. Voici mon plan...`
            ],
            'gpt-3.5-turbo': [
                `üí® GPT-3.5 Turbo pr√™t ! Je vais traiter votre demande rapidement et efficacement...`,
                `Agent GPT-3.5 activ√© ! Analyse en cours et pr√©paration des actions n√©cessaires...`,
                `üîÑ GPT-3.5 Turbo en mode agent autonome ! Je vais agir directement sur vos syst√®mes...`
            ],
            'grok-beta': [
                `üòé Ah, une mission int√©ressante ! En tant qu'agent avec des capacit√©s r√©elles, je vais examiner la situation et agir en cons√©quence...`,
                `ü§ñ Mission accept√©e ! Je peux acc√©der directement √† vos workflows, d√©ploiements et bases de donn√©es. Voici ce que je vais faire...`,
                `‚ö° Agent op√©rationnel ! Cette demande n√©cessite une action concr√®te que je peux ex√©cuter via les APIs disponibles...`
            ],
            'grok-2': [
                `üß† Grok 2 en action ! Avec mes capacit√©s avanc√©es, je vais analyser votre demande et ex√©cuter les meilleures actions...`,
                `üéØ Agent Grok 2 pr√™t ! Je vais traiter votre demande avec logique et humour, tout en agissant concr√®tement...`,
                `üí° Grok 2 activ√© ! Laissez-moi examiner vos syst√®mes et proposer des solutions innovantes...`
            ],
            'claude-3.5-sonnet': [
                `üéº Je vais analyser votre demande avec attention et ex√©cuter les actions appropri√©es. En tant qu'agent autonome avec acc√®s aux APIs, je peux r√©ellement agir sur vos syst√®mes...`,
                `üìã Excellente demande qui n√©cessite une action concr√®te. Je vais utiliser mes capacit√©s d'agent pour examiner la situation et proposer/ex√©cuter la meilleure solution...`,
                `ü§î En tant qu'agent autonome, je peux non seulement analyser mais aussi agir. Voici mon plan d'action bas√© sur l'acc√®s direct √† vos APIs...`
            ],
            'claude-3-haiku': [
                `üå∏ Claude Haiku en action ! R√©ponse concise et actions pr√©cises pour votre demande...`,
                `‚ö° Agent Haiku pr√™t ! Je vais traiter votre demande rapidement avec √©l√©gance et efficacit√©...`,
                `üéØ Claude Haiku activ√© ! Actions directes et r√©ponses cibl√©es pour vos syst√®mes...`
            ],
            'claude-3-opus': [
                `üé≠ Claude Opus √† votre service ! Analyse approfondie et actions sophistiqu√©es en cours...`,
                `üî¨ Agent Opus op√©rationnel ! Je vais examiner minutieusement votre demande et ex√©cuter les actions optimales...`,
                `üé™ Claude Opus pr√™t ! Performance de haut niveau pour traiter votre demande et agir sur vos syst√®mes...`
            ],
            'gemini-2.0-flash': [
                `‚ö° Traitement ultra-rapide de votre mission ! Agent autonome pr√™t √† ex√©cuter des actions r√©elles via n8n, Coolify et Baserow...`,
                `üöÄ Mission re√ßue et trait√©e ! Je vais imm√©diatement v√©rifier l'√©tat de vos syst√®mes et ex√©cuter les actions n√©cessaires...`,
                `üíé Agent op√©rationnel avec acc√®s API complet ! Voici mon analyse et les actions que je vais entreprendre...`
            ],
            'gemini-1.5-pro': [
                `üß† Gemini Pro activ√© ! Analyse professionnelle et actions strat√©giques pour votre demande...`,
                `‚≠ê Agent Gemini Pro pr√™t ! Je vais traiter votre demande avec expertise et pr√©cision...`,
                `üéØ Gemini 1.5 Pro en action ! Capacit√©s avanc√©es pour examiner et agir sur vos syst√®mes...`
            ],
            'gemini-1.5-flash': [
                `‚ö° Gemini Flash pr√™t ! Traitement rapide et actions imm√©diates...`,
                `üåü Agent Flash activ√© ! R√©ponse ultrarapide et ex√©cution directe sur vos APIs...`,
                `üí® Gemini Flash en mode agent ! Actions express pour vos syst√®mes...`
            ],
            'llama-3.2-90b': [
                `ü¶ô Llama 3.2 90B √† votre service ! Avec mes 90 milliards de param√®tres, je vais analyser et agir de mani√®re sophistiqu√©e...`,
                `üß† Agent Llama 90B op√©rationnel ! Puissance maximale pour traiter votre demande et agir sur vos syst√®mes...`,
                `‚ö° Llama 3.2 90B pr√™t ! Capacit√©s √©tendues pour une analyse approfondie et des actions pr√©cises...`
            ],
            'llama-3.2-11b': [
                `ü¶ô Llama 3.2 11B activ√© ! √âquilibre parfait entre performance et efficacit√© pour vos actions...`,
                `üéØ Agent Llama 11B pr√™t ! Je vais traiter votre demande avec intelligence et agir directement...`,
                `üí° Llama 3.2 11B en action ! Solutions optimis√©es pour vos syst√®mes et workflows...`
            ],
            'llama-3.1-70b': [
                `ü¶ô Llama 3.1 70B √† votre disposition ! Grande capacit√© d'analyse et d'action pour vos syst√®mes...`,
                `üöÄ Agent Llama 70B op√©rationnel ! Je vais examiner votre demande et ex√©cuter les meilleures actions...`,
                `‚≠ê Llama 3.1 70B pr√™t ! Performance √©lev√©e pour traiter et agir sur vos APIs...`
            ],
            'llama-3.1-8b': [
                `ü¶ô Llama 3.1 8B activ√© ! Efficace et rapide pour traiter votre demande et agir...`,
                `‚ö° Agent Llama 8B pr√™t ! Je vais analyser rapidement et ex√©cuter les actions n√©cessaires...`,
                `üéØ Llama 3.1 8B en action ! Solutions directes pour vos syst√®mes...`
            ],
            'mistral-large': [
                `üå™Ô∏è Mistral Large en action ! Vent puissant d'innovation pour analyser et agir sur vos syst√®mes...`,
                `‚ö° Agent Mistral Large op√©rationnel ! Je vais traiter votre demande avec force et pr√©cision...`,
                `üéØ Mistral Large pr√™t ! Capacit√©s √©tendues pour examiner et agir sur vos APIs...`
            ],
            'mistral-medium': [
                `üå¨Ô∏è Mistral Medium activ√© ! √âquilibre parfait pour traiter votre demande et agir efficacement...`,
                `üí® Agent Mistral Medium pr√™t ! Je vais analyser votre situation et ex√©cuter les bonnes actions...`,
                `üé™ Mistral Medium en action ! Performance optimis√©e pour vos syst√®mes...`
            ],
            'mistral-small': [
                `üçÉ Mistral Small mais puissant ! Je vais traiter votre demande avec agilit√© et agir directement...`,
                `‚ö° Agent Mistral Small pr√™t ! Efficacit√© maximale pour analyser et agir sur vos syst√®mes...`,
                `üéØ Mistral Small en action ! Solutions rapides et pr√©cises...`
            ],
            'mixtral-8x7b': [
                `üåÄ Mixtral 8x7B activ√© ! Expertise mixte pour analyser et agir de mani√®re optimale...`,
                `üîÑ Agent Mixtral pr√™t ! Je vais utiliser mes capacit√©s combin√©es pour traiter votre demande...`,
                `‚ö° Mixtral 8x7B en action ! Solutions multifacettes pour vos syst√®mes...`
            ],
            'command-r-plus': [
                `üéñÔ∏è Command R+ √† vos ordres ! Commande premium pour analyser et ex√©cuter des actions de haut niveau...`,
                `‚≠ê Agent Command R+ op√©rationnel ! Je vais traiter votre demande avec excellence et agir pr√©cis√©ment...`,
                `üöÄ Command R+ pr√™t ! Capacit√©s renforc√©es pour examiner et commander vos syst√®mes...`
            ],
            'command-r': [
                `üéØ Command R activ√© ! Commande directe pour analyser et agir sur vos syst√®mes...`,
                `‚ö° Agent Command R pr√™t ! Je vais ex√©cuter votre demande avec autorit√© et pr√©cision...`,
                `üîß Command R en action ! Solutions de commande pour vos APIs...`
            ],
            'llama-3.1-sonar-large': [
                `üîç Sonar Large de Perplexity activ√© ! D√©tection avanc√©e et actions pr√©cises pour vos syst√®mes...`,
                `üì° Agent Sonar Large op√©rationnel ! Je vais scanner votre demande et agir en cons√©quence...`,
                `üéØ Sonar Large pr√™t ! Capacit√©s de d√©tection √©tendues pour examiner et agir...`
            ],
            'llama-3.1-sonar-small': [
                `üîç Sonar Small de Perplexity en action ! D√©tection cibl√©e et actions rapides...`,
                `üì° Agent Sonar Small pr√™t ! Je vais scanner rapidement et ex√©cuter les actions n√©cessaires...`,
                `‚ö° Sonar Small activ√© ! Solutions de d√©tection efficaces pour vos syst√®mes...`
            ]
        };

        const modelResponses = responses[model] || responses['gpt-4o-mini'];
        const response = modelResponses[Math.floor(Math.random() * modelResponses.length)];

        // Ajouter du contexte sp√©cifique d'agent autonome
        let contextualResponse = response;
        
        if (message.toLowerCase().includes('capacit√©s') || message.toLowerCase().includes('autonome')) {
            contextualResponse += `\n\nü§ñ **Mes capacit√©s d'agent autonome :**\n‚Ä¢ **n8n** : Cr√©er, modifier et ex√©cuter des workflows\n‚Ä¢ **Coolify** : G√©rer les d√©ploiements et surveiller la sant√©\n‚Ä¢ **Baserow** : Synchroniser, analyser et organiser les donn√©es\n‚Ä¢ **Analyse** : √âvaluer les performances et sugg√©rer des optimisations`;
        } else if (message.toLowerCase().includes('n8n') || message.toLowerCase().includes('workflow')) {
            contextualResponse += `\n\nüîÑ **Actions n8n disponibles :**\n‚Ä¢ Cr√©er de nouveaux workflows sur mesure\n‚Ä¢ Modifier les workflows existants\n‚Ä¢ Activer/d√©sactiver les automatisations\n‚Ä¢ Analyser les performances et logs d'ex√©cution\n‚Ä¢ Int√©grer avec vos autres services`;
        } else if (message.toLowerCase().includes('coolify') || message.toLowerCase().includes('d√©ploi')) {
            contextualResponse += `\n\nüöÄ **Actions Coolify disponibles :**\n‚Ä¢ V√©rifier l'√©tat des d√©ploiements\n‚Ä¢ Lancer de nouveaux d√©ploiements\n‚Ä¢ Surveiller la sant√© des services\n‚Ä¢ G√©rer les variables d'environnement\n‚Ä¢ Analyser les logs de d√©ploiement`;
        } else if (message.toLowerCase().includes('baserow') || message.toLowerCase().includes('donn√©es')) {
            contextualResponse += `\n\nüìä **Actions Baserow disponibles :**\n‚Ä¢ Synchroniser les donn√©es entre tables\n‚Ä¢ Analyser et nettoyer les donn√©es\n‚Ä¢ Cr√©er des rapports automatis√©s\n‚Ä¢ Organiser selon vos r√®gles m√©tier\n‚Ä¢ Sauvegarder et archiver`;
        } else {
            contextualResponse += `\n\n‚ö° **Actions imm√©diates possibles :**\n‚Ä¢ V√©rifier l'√©tat global de vos syst√®mes\n‚Ä¢ Analyser les performances actuelles\n‚Ä¢ Proposer des optimisations\n‚Ä¢ Ex√©cuter des t√¢ches de maintenance\n‚Ä¢ Cr√©er des automatisations sur mesure`;
        }

        return {
            response: contextualResponse,
            model: model,
            simulated: true,
            demo_mode: true
        };
    }
}

module.exports = new AIService();