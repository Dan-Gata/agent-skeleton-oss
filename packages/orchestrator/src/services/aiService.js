// Service IA r√©el avec int√©gration des APIs
const axios = require('axios');

class AIService {
    constructor() {
        // Configuration des APIs
        this.config = {
            openai: {
                apiKey: process.env.OPENAI_API_KEY,
                baseURL: 'https://api.openai.com/v1'
            },
            anthropic: {
                apiKey: process.env.ANTHROPIC_API_KEY,
                baseURL: 'https://api.anthropic.com/v1'
            },
            google: {
                apiKey: process.env.GOOGLE_API_KEY,
                baseURL: 'https://generativelanguage.googleapis.com/v1beta'
            },
            openrouter: {
                apiKey: process.env.OPENROUTER_API_KEY,
                baseURL: 'https://openrouter.ai/api/v1'
            }
        };
    }

    async sendMessage(message, model = 'gpt-4o-mini', conversationId = null) {
        try {
            console.log(`ü§ñ Envoi message √† ${model}:`, message);

            // Router vers la bonne API selon le mod√®le
            if (model.includes('gpt') || model.includes('openai')) {
                return await this.callOpenAI(message, model);
            } else if (model.includes('claude') || model.includes('anthropic')) {
                return await this.callAnthropic(message, model);
            } else if (model.includes('gemini') || model.includes('google')) {
                return await this.callGoogle(message, model);
            } else if (model.includes('grok') || this.config.openrouter.apiKey) {
                return await this.callOpenRouter(message, model);
            } else {
                // Fallback vers simulation am√©lior√©e
                return await this.simulateResponse(message, model);
            }
        } catch (error) {
            console.error('‚ùå Erreur service IA:', error);
            return {
                response: `‚ùå Erreur de connexion avec ${model}. ${error.message}`,
                model: model,
                error: true
            };
        }
    }

    async callOpenAI(message, model) {
        if (!this.config.openai.apiKey) {
            throw new Error('Cl√© API OpenAI manquante');
        }

        const response = await axios.post(
            `${this.config.openai.baseURL}/chat/completions`,
            {
                model: model === 'gpt-4o-mini' ? 'gpt-4o-mini' : 'gpt-4',
                messages: [
                    {
                        role: 'system',
                        content: 'Tu es un assistant IA intelligent int√©gr√© dans Agent Skeleton OSS. Tu aides avec le d√©veloppement, l\'automatisation, et les int√©grations. R√©ponds de mani√®re utile et concise.'
                    },
                    {
                        role: 'user',
                        content: message
                    }
                ],
                max_tokens: 1000,
                temperature: 0.7
            },
            {
                headers: {
                    'Authorization': `Bearer ${this.config.openai.apiKey}`,
                    'Content-Type': 'application/json'
                }
            }
        );

        return {
            response: response.data.choices[0].message.content,
            model: model,
            usage: response.data.usage
        };
    }

    async callAnthropic(message, model) {
        if (!this.config.anthropic.apiKey) {
            throw new Error('Cl√© API Anthropic manquante');
        }

        const response = await axios.post(
            `${this.config.anthropic.baseURL}/messages`,
            {
                model: model.includes('3.5') ? 'claude-3-5-sonnet-20241022' : 'claude-3-haiku-20240307',
                max_tokens: 1000,
                messages: [
                    {
                        role: 'user',
                        content: message
                    }
                ],
                system: 'Tu es un assistant IA intelligent int√©gr√© dans Agent Skeleton OSS. Tu aides avec le d√©veloppement, l\'automatisation, et les int√©grations. R√©ponds de mani√®re utile et concise.'
            },
            {
                headers: {
                    'x-api-key': this.config.anthropic.apiKey,
                    'Content-Type': 'application/json',
                    'anthropic-version': '2023-06-01'
                }
            }
        );

        return {
            response: response.data.content[0].text,
            model: model,
            usage: response.data.usage
        };
    }

    async callGoogle(message, model) {
        if (!this.config.google.apiKey) {
            throw new Error('Cl√© API Google manquante');
        }

        const response = await axios.post(
            `${this.config.google.baseURL}/models/gemini-1.5-flash:generateContent?key=${this.config.google.apiKey}`,
            {
                contents: [
                    {
                        parts: [
                            {
                                text: `Tu es un assistant IA intelligent int√©gr√© dans Agent Skeleton OSS. Tu aides avec le d√©veloppement, l'automatisation, et les int√©grations. Voici la question de l'utilisateur: ${message}`
                            }
                        ]
                    }
                ]
            },
            {
                headers: {
                    'Content-Type': 'application/json'
                }
            }
        );

        return {
            response: response.data.candidates[0].content.parts[0].text,
            model: model,
            usage: response.data.usageMetadata
        };
    }

    async callOpenRouter(message, model) {
        if (!this.config.openrouter.apiKey) {
            throw new Error('Cl√© API OpenRouter manquante');
        }

        // Mapping des mod√®les vers OpenRouter
        const modelMap = {
            'grok-beta': 'x-ai/grok-beta',
            'grok-2': 'x-ai/grok-2',
            'claude-3.5-sonnet': 'anthropic/claude-3.5-sonnet',
            'gemini-2.0-flash': 'google/gemini-2.0-flash-exp',
            'gpt-4o-mini': 'openai/gpt-4o-mini'
        };

        const routerModel = modelMap[model] || model;

        const response = await axios.post(
            `${this.config.openrouter.baseURL}/chat/completions`,
            {
                model: routerModel,
                messages: [
                    {
                        role: 'system',
                        content: 'Tu es un assistant IA intelligent int√©gr√© dans Agent Skeleton OSS. Tu aides avec le d√©veloppement, l\'automatisation, et les int√©grations. R√©ponds de mani√®re utile et concise.'
                    },
                    {
                        role: 'user',
                        content: message
                    }
                ],
                max_tokens: 1000,
                temperature: 0.7
            },
            {
                headers: {
                    'Authorization': `Bearer ${this.config.openrouter.apiKey}`,
                    'Content-Type': 'application/json',
                    'HTTP-Referer': process.env.APP_URL || 'http://localhost:3000',
                    'X-Title': 'Agent Skeleton OSS'
                }
            }
        );

        return {
            response: response.data.choices[0].message.content,
            model: model,
            usage: response.data.usage
        };
    }

    async simulateResponse(message, model) {
        // R√©ponses simul√©es am√©lior√©es en attendant les cl√©s API
        const responses = {
            'gpt-4o-mini': [
                `üöÄ GPT-4o Mini : Excellente question ! Pour ${message.toLowerCase().includes('n8n') ? 'n8n' : message.toLowerCase().includes('coolify') ? 'Coolify' : 'cela'}, je recommande une approche structur√©e...`,
                `‚ö° GPT-4o Mini : Bas√© sur votre demande concernant "${message.substring(0, 50)}...", voici mon analyse d√©taill√©e...`,
                `üéØ GPT-4o Mini : Je comprends votre besoin. Pour optimiser cette solution, nous pourrions...`
            ],
            'grok-beta': [
                `ü§ñ Grok : Ah, une question int√©ressante ! Laissez-moi vous donner ma perspective unique sur "${message.substring(0, 30)}..."`,
                `‚ö° Grok Fast : Traitement rapide de votre demande ! Voici une approche efficace pour r√©soudre cela...`,
                `üî• Grok : D'apr√®s mon analyse, la meilleure strat√©gie serait...`
            ],
            'claude-3.5-sonnet': [
                `üß† Claude 3.5 Sonnet : Je vais analyser votre demande avec attention. Concernant "${message.substring(0, 40)}...", voici ma r√©flexion structur√©e...`,
                `üí° Claude 3.5 : Excellente question qui m√©rite une r√©ponse nuanc√©e. Permettez-moi de d√©composer cela...`,
                `üìä Claude 3.5 Sonnet : Apr√®s analyse de votre demande, je propose cette approche m√©thodique...`
            ],
            'gemini-2.0-flash': [
                `üíé Gemini 2.0 Flash : Traitement ultra-rapide ! Pour votre question sur "${message.substring(0, 35)}...", voici ma r√©ponse optimis√©e...`,
                `üåü Gemini Flash : Analyse multimodale en cours... Voici une solution compl√®te pour votre demande...`,
                `‚ö° Gemini 2.0 : R√©ponse instantan√©e ! Bas√© sur les derni√®res donn√©es, je recommande...`
            ]
        };

        const modelResponses = responses[model] || responses['gpt-4o-mini'];
        const response = modelResponses[Math.floor(Math.random() * modelResponses.length)];

        // Ajouter du contexte sp√©cifique
        let contextualResponse = response;
        if (message.toLowerCase().includes('n8n')) {
            contextualResponse += `\n\nüîó Pour n8n sp√©cifiquement : cr√©ez un webhook trigger, ajoutez une condition de filtrage, puis connectez √† votre API de destination. N'oubliez pas de tester en mode debug !`;
        } else if (message.toLowerCase().includes('coolify')) {
            contextualResponse += `\n\nüöÄ Pour Coolify : v√©rifiez vos variables d'environnement, assurez-vous que le Dockerfile est optimis√©, et surveillez les logs de d√©ploiement.`;
        } else if (message.toLowerCase().includes('baserow')) {
            contextualResponse += `\n\nüìä Pour Baserow : utilisez l'API REST, configurez les webhooks pour les mises √† jour en temps r√©el, et pensez √† la structure de vos tables.`;
        }

        return {
            response: contextualResponse,
            model: model,
            simulated: true
        };
    }
}

module.exports = new AIService();