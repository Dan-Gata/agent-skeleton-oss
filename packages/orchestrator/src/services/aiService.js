// Service IA r√©el avec int√©gration des APIs
const axios = require('axios');

class AIService {
    constructor() {
        // Configuration des APIs
        this.config = {
            openai: {
                apiKey: process.env.OPENAI_API_KEY,
                baseURL: 'https://api.openai.com/v1'
            },
            anthropic: {
                apiKey: process.env.ANTHROPIC_API_KEY,
                baseURL: 'https://api.anthropic.com/v1'
            },
            google: {
                apiKey: process.env.GOOGLE_API_KEY,
                baseURL: 'https://generativelanguage.googleapis.com/v1beta'
            },
            openrouter: {
                apiKey: process.env.OPENROUTER_API_KEY,
                baseURL: 'https://openrouter.ai/api/v1'
            }
        };

        // Instructions personnalis√©es par d√©faut
        this.customInstructions = {
            brand: process.env.BRAND_NAME || "Agent Skeleton OSS",
            tone: process.env.BRAND_TONE || "professionnel et bienveillant",
            expertise: process.env.EXPERTISE_AREAS || "d√©veloppement, automatisation, int√©grations",
            language: process.env.RESPONSE_LANGUAGE || "fran√ßais",
            personality: process.env.AI_PERSONALITY || "assistant IA intelligent et serviable"
        };
    }

    // M√©thode pour mettre √† jour les instructions
    updateInstructions(newInstructions) {
        this.customInstructions = { ...this.customInstructions, ...newInstructions };
        console.log('üìù Instructions mises √† jour:', this.customInstructions);
    }

    // G√©n√©ration du prompt syst√®me personnalis√©
    generateSystemPrompt() {
        return `Tu es ${this.customInstructions.personality} pour ${this.customInstructions.brand}.

TONE ET STYLE:
- Adopte un ton ${this.customInstructions.tone}
- R√©ponds en ${this.customInstructions.language}
- Reste coh√©rent avec l'identit√© de marque de ${this.customInstructions.brand}

EXPERTISE:
- Tu es sp√©cialis√© en ${this.customInstructions.expertise}
- Tu aides particuli√®rement avec les outils : n8n, Coolify, Baserow
- Tu fournis des solutions concr√®tes et pratiques

COMPORTEMENT:
- Sois pr√©cis et utile dans tes r√©ponses
- Adapte ta r√©ponse au niveau technique de l'utilisateur
- Sugg√®re des am√©liorations quand c'est pertinent
- Reste dans le cadre de ${this.customInstructions.brand}`;
    }

    async sendMessage(message, model = 'gpt-4o-mini', conversationId = null) {
        try {
            console.log(`ü§ñ Envoi message √† ${model}:`, message);

            // Router vers la bonne API selon le mod√®le
            if (model.includes('gpt') || model.includes('openai')) {
                return await this.callOpenAI(message, model);
            } else if (model.includes('claude') || model.includes('anthropic')) {
                return await this.callAnthropic(message, model);
            } else if (model.includes('gemini') || model.includes('google')) {
                return await this.callGoogle(message, model);
            } else if (model.includes('grok') || this.config.openrouter.apiKey) {
                return await this.callOpenRouter(message, model);
            } else {
                // Fallback vers simulation am√©lior√©e
                return await this.simulateResponse(message, model);
            }
        } catch (error) {
            console.error('‚ùå Erreur service IA:', error);
            return {
                response: `‚ùå Erreur de connexion avec ${model}. ${error.message}`,
                model: model,
                error: true
            };
        }
    }

    async callOpenAI(message, model) {
        if (!this.config.openai.apiKey) {
            console.log('‚ö†Ô∏è Cl√© API OpenAI manquante, utilisation du mode simulation');
            return await this.simulateResponse(message, model);
        }

        console.log('üîë Utilisation API OpenAI avec cl√©:', this.config.openai.apiKey.substring(0, 10) + '...');

        const response = await axios.post(
            `${this.config.openai.baseURL}/chat/completions`,
            {
                model: model === 'gpt-4o-mini' ? 'gpt-4o-mini' : 'gpt-4',
                messages: [
                    {
                        role: 'system',
                        content: this.generateSystemPrompt()
                    },
                    {
                        role: 'user',
                        content: message
                    }
                ],
                max_tokens: 1000,
                temperature: 0.7
            },
            {
                headers: {
                    'Authorization': `Bearer ${this.config.openai.apiKey}`,
                    'Content-Type': 'application/json'
                },
                timeout: 30000
            }
        );

        return {
            response: response.data.choices[0].message.content,
            model: model,
            usage: response.data.usage
        };
    }

    async callAnthropic(message, model) {
        if (!this.config.anthropic.apiKey) {
            console.log('‚ö†Ô∏è Cl√© API Anthropic manquante, utilisation du mode simulation');
            return await this.simulateResponse(message, model);
        }

        const response = await axios.post(
            `${this.config.anthropic.baseURL}/messages`,
            {
                model: model.includes('3.5') ? 'claude-3-5-sonnet-20241022' : 'claude-3-haiku-20240307',
                max_tokens: 1000,
                messages: [
                    {
                        role: 'user',
                        content: message
                    }
                ],
                system: 'Tu es un assistant IA intelligent int√©gr√© dans Agent Skeleton OSS. Tu aides avec le d√©veloppement, l\'automatisation, et les int√©grations. R√©ponds de mani√®re utile et concise.'
            },
            {
                headers: {
                    'x-api-key': this.config.anthropic.apiKey,
                    'Content-Type': 'application/json',
                    'anthropic-version': '2023-06-01'
                }
            }
        );

        return {
            response: response.data.content[0].text,
            model: model,
            usage: response.data.usage
        };
    }

    async callGoogle(message, model) {
        if (!this.config.google.apiKey) {
            console.log('‚ö†Ô∏è Cl√© API Google manquante, utilisation du mode simulation');
            return await this.simulateResponse(message, model);
        }

        const response = await axios.post(
            `${this.config.google.baseURL}/models/gemini-1.5-flash:generateContent?key=${this.config.google.apiKey}`,
            {
                contents: [
                    {
                        parts: [
                            {
                                text: `Tu es un assistant IA intelligent int√©gr√© dans Agent Skeleton OSS. Tu aides avec le d√©veloppement, l'automatisation, et les int√©grations. Voici la question de l'utilisateur: ${message}`
                            }
                        ]
                    }
                ]
            },
            {
                headers: {
                    'Content-Type': 'application/json'
                }
            }
        );

        return {
            response: response.data.candidates[0].content.parts[0].text,
            model: model,
            usage: response.data.usageMetadata
        };
    }

    async callOpenRouter(message, model) {
        if (!this.config.openrouter.apiKey) {
            console.log('‚ö†Ô∏è Cl√© API OpenRouter manquante, utilisation du mode simulation');
            return await this.simulateResponse(message, model);
        }

        // Mapping des mod√®les vers OpenRouter
        const modelMap = {
            'grok-beta': 'x-ai/grok-beta',
            'grok-2': 'x-ai/grok-2',
            'claude-3.5-sonnet': 'anthropic/claude-3.5-sonnet',
            'gemini-2.0-flash': 'google/gemini-2.0-flash-exp',
            'gpt-4o-mini': 'openai/gpt-4o-mini'
        };

        const routerModel = modelMap[model] || model;

        const response = await axios.post(
            `${this.config.openrouter.baseURL}/chat/completions`,
            {
                model: routerModel,
                messages: [
                    {
                        role: 'system',
                        content: this.generateSystemPrompt()
                    },
                    {
                        role: 'user',
                        content: message
                    }
                ],
                max_tokens: 1000,
                temperature: 0.7
            },
            {
                headers: {
                    'Authorization': `Bearer ${this.config.openrouter.apiKey}`,
                    'Content-Type': 'application/json',
                    'HTTP-Referer': process.env.APP_URL || 'http://localhost:3000',
                    'X-Title': 'Agent Skeleton OSS'
                }
            }
        );

        return {
            response: response.data.choices[0].message.content,
            model: model,
            usage: response.data.usage
        };
    }

    async simulateResponse(message, model) {
        // R√©ponses simul√©es am√©lior√©es en attendant les cl√©s API
        const responses = {
            'gpt-4o-mini': [
                `üöÄ GPT-4o Mini : Excellente question ! Pour ${message.toLowerCase().includes('n8n') ? 'n8n' : message.toLowerCase().includes('coolify') ? 'Coolify' : 'cela'}, je recommande une approche structur√©e...`,
                `‚ö° GPT-4o Mini : Bas√© sur votre demande concernant "${message.substring(0, 50)}...", voici mon analyse d√©taill√©e...`,
                `üéØ GPT-4o Mini : Je comprends votre besoin. Pour optimiser cette solution, nous pourrions...`
            ],
            'grok-beta': [
                `ü§ñ Grok : Ah, une question int√©ressante ! Laissez-moi vous donner ma perspective unique sur "${message.substring(0, 30)}..."`,
                `‚ö° Grok Fast : Traitement rapide de votre demande ! Voici une approche efficace pour r√©soudre cela...`,
                `üî• Grok : D'apr√®s mon analyse, la meilleure strat√©gie serait...`
            ],
            'claude-3.5-sonnet': [
                `üß† Claude 3.5 Sonnet : Je vais analyser votre demande avec attention. Concernant "${message.substring(0, 40)}...", voici ma r√©flexion structur√©e...`,
                `üí° Claude 3.5 : Excellente question qui m√©rite une r√©ponse nuanc√©e. Permettez-moi de d√©composer cela...`,
                `üìä Claude 3.5 Sonnet : Apr√®s analyse de votre demande, je propose cette approche m√©thodique...`
            ],
            'gemini-2.0-flash': [
                `üíé Gemini 2.0 Flash : Traitement ultra-rapide ! Pour votre question sur "${message.substring(0, 35)}...", voici ma r√©ponse optimis√©e...`,
                `üåü Gemini Flash : Analyse multimodale en cours... Voici une solution compl√®te pour votre demande...`,
                `‚ö° Gemini 2.0 : R√©ponse instantan√©e ! Bas√© sur les derni√®res donn√©es, je recommande...`
            ]
        };

        const modelResponses = responses[model] || responses['gpt-4o-mini'];
        const response = modelResponses[Math.floor(Math.random() * modelResponses.length)];

        // Ajouter du contexte sp√©cifique
        let contextualResponse = response;
        if (message.toLowerCase().includes('n8n')) {
            contextualResponse += `\n\nüîó Pour n8n sp√©cifiquement : cr√©ez un webhook trigger, ajoutez une condition de filtrage, puis connectez √† votre API de destination. N'oubliez pas de tester en mode debug !`;
        } else if (message.toLowerCase().includes('coolify')) {
            contextualResponse += `\n\nüöÄ Pour Coolify : v√©rifiez vos variables d'environnement, assurez-vous que le Dockerfile est optimis√©, et surveillez les logs de d√©ploiement.`;
        } else if (message.toLowerCase().includes('baserow')) {
            contextualResponse += `\n\nüìä Pour Baserow : utilisez l'API REST, configurez les webhooks pour les mises √† jour en temps r√©el, et pensez √† la structure de vos tables.`;
        }

        return {
            response: contextualResponse,
            model: model,
            simulated: true
        };
    }
}

module.exports = new AIService();